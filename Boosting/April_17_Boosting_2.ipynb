{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d128a6fc-1e7c-44d7-a66f-36f769ec36e3",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5240fb-7e3a-4bcf-ac52-41564f9d4b96",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for building predictive models, primarily for regression problems. It is a type of boosting algorithm that builds an ensemble of decision tree models sequentially, with each subsequent tree model attempting to correct the errors of the previous tree.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. A decision tree model is trained on the training data to predict the target variable.\n",
    "\n",
    "2. The errors of the first model are computed by comparing its predictions to the actual target values.\n",
    "\n",
    "3. A second decision tree model is trained on the training data, but this time the target variable is adjusted by the negative gradient of the loss function with respect to the output of the previous model. This means that the second model tries to predict the residual errors of the first model, rather than the original target variable.\n",
    "\n",
    "4. The predicted values of the second model are added to the predictions of the first model, resulting in a more accurate overall prediction.\n",
    "\n",
    "5. Steps 2-4 are repeated for a predetermined number of iterations, each time training a new decision tree model on the residual errors of the previous models and adding its predicted values to the overall prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1218b-09cb-4398-8ca6-02120f5b003d",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17411a-b3bd-44d2-aeb8-beb9664a80d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15e9f91e-22b8-4ed1-af28-362fcdfab5af",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97a39d-5f54-476a-b619-55663630ef37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d0c7c14-91b2-47fd-b432-c71143333c28",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21fb33-f96c-4b4a-bc4e-35d758b65b7f",
   "metadata": {},
   "source": [
    "In gradient boosting, a weak learner is a machine learning model that is only slightly better than random guessing. A weak learner is typically a model with low complexity, such as a decision tree with few splits or a linear model with few features. The idea behind gradient boosting is to combine a sequence of weak learners into a single strong learner, by iteratively fitting the weak learners to the residual errors of the previous models.\n",
    "\n",
    "Each weak learner in gradient boosting is trained on the residuals of the previous models, so that it focuses on the patterns that were not captured by the previous models. By doing this iteratively, the overall model can improve its performance by combining the strengths of the individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81634b-dabf-4fca-963a-602f76dc5414",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414e319-2fa3-47eb-b4b1-b4772592f8fe",
   "metadata": {},
   "source": [
    "The intuition behind gradient boosting is to improve the accuracy of a machine learning model by iteratively fitting weak learners to the residual errors of the previous models. In other words, gradient boosting works by combining a sequence of simple models, each one designed to correct the errors of the previous models, until a strong model is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc377e17-d15b-49cb-9928-4acd538797cd",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3e258-d3d1-4a1e-a38d-5e5aa2ddc9b9",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively training a sequence of models, where each model is designed to correct the errors of the previous models. The process of building the ensemble can be summarized in the following steps:\n",
    "\n",
    "1. Initialize the ensemble with a simple model: The first model in the ensemble is typically a simple model, such as a decision tree with a small number of splits. This model is used to make initial predictions on the training data.\n",
    "\n",
    "2. Compute the residual errors: The difference between the actual target values and the predictions of the current model are computed to obtain the residual errors. These residuals represent the errors that were not captured by the previous models in the ensemble.\n",
    "\n",
    "3. Train a new model to correct the residual errors: A new model is trained to predict the residual errors of the previous model. This new model is trained on the original features of the data, but with the target values replaced by the residual errors.\n",
    "\n",
    "4. Add the new model to the ensemble: The new model is added to the ensemble of weak learners, with a weight that represents its contribution to the final prediction.\n",
    "\n",
    "5. Update the ensemble predictions: The ensemble's predictions are updated by adding the predictions of the new model, scaled by a learning rate, to the predictions of the previous models.\n",
    "\n",
    "6. Repeat steps 2-5 for a fixed number of iterations: The process of computing the residuals, training a new model, adding it to the ensemble, and updating the predictions is repeated for a fixed number of iterations, typically in the range of 100-1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a80f9-df3e-4fc8-8276-bf7902a559c7",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6674187-606d-49e1-87d9-25ad274fec87",
   "metadata": {},
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm can be understood by considering the following steps:\n",
    "\n",
    "1. Define a loss function: The first step in constructing the mathematical intuition of Gradient Boosting is to define a loss function that measures the difference between the predicted values and the true values of the data. The most commonly used loss function in regression problems is the mean squared error (MSE) loss, which is defined as the average of the squared differences between the predicted values and the true values.\n",
    "\n",
    "2. Fit a base model: The second step is to fit a base model, which can be a simple model such as a decision tree with a small number of splits. This model is used to make initial predictions on the training data.\n",
    "\n",
    "3. Compute the negative gradient of the loss function: The negative gradient of the loss function with respect to the predicted values is computed. This represents the direction in which the loss function is decreasing the fastest. By computing the negative gradient, we can determine how much we need to adjust the predictions of the base model to reduce the loss function.\n",
    "\n",
    "4. Fit a new model to the negative gradient: A new model is trained to predict the negative gradient of the loss function. This model is trained on the original features of the data, but with the target values replaced by the negative gradient of the loss function.\n",
    "\n",
    "5. Update the predictions: The predictions of the base model are updated by adding the predictions of the new model, scaled by a learning rate, to the predictions of the base model. This update is designed to move the predictions in the direction of the negative gradient, and hence reduce the loss function.\n",
    "\n",
    "6. Repeat steps 3-5 for a fixed number of iterations: The process of computing the negative gradient, training a new model, and updating the predictions is repeated for a fixed number of iterations, typically in the range of 100-1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31b2ff6-4cd7-4036-9f54-684fcaf584a2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

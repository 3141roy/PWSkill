{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18638e2-c9e4-41f2-b13a-4eb109aa588e",
   "metadata": {},
   "source": [
    "### Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895a162-f335-444f-8b57-cda8f195d78e",
   "metadata": {},
   "source": [
    "Ensemble technique is a special kind of technique in Machine Learning where the data is trained on multiple different models and the final result is taken as the aggregate of the results obtained from all these models, mode value in case of Classifiers and mean in case of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0608e-3154-4d43-8166-911cf654ecf0",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf236f-2682-4916-bfa9-593ea6ce8e6b",
   "metadata": {},
   "source": [
    "\n",
    "Ensemble learning is used in machine learning for several reasons:\n",
    "\n",
    "1. Improved accuracy: Ensemble learning can improve the accuracy of a model by combining the predictions of multiple models, each of which may have different strengths and weaknesses.\n",
    "\n",
    "2. Increased stability: Ensemble learning can help to reduce the variance of a model by combining the predictions of multiple models trained on different subsets of the data. This can help to make the model more robust and less prone to overfitting.\n",
    "\n",
    "3. Generalization: Ensemble learning can help to improve the generalization of a model by reducing the impact of noise or outliers in the data. By combining the predictions of multiple models, the ensemble can learn to ignore noise and focus on the underlying patterns in the data.\n",
    "\n",
    "4. Model selection: Ensemble learning can be used to combine the predictions of multiple models trained with different hyperparameters or architectures. This can help to identify the best model for a given task and improve the overall performance of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50652b50-dd4b-4e09-b21c-299cce74a008",
   "metadata": {},
   "source": [
    "### Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbe1d8-3da5-4ef4-af6f-3e7cfbe8511c",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is a machine learning ensemble technique that involves training multiple instances of the same model on different subsets of the training data and then combining their predictions to produce a final prediction.\n",
    "\n",
    "The bagging technique is used to reduce the variance of a single model by introducing randomness into the training process. To do this, the training dataset is randomly sampled with replacement to create multiple bootstrap samples, which are used to train multiple instances of the same model. Each model is trained on a different bootstrap sample, and the final prediction is made by aggregating the predictions of all models, typically by taking the average or majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5aa30-582c-4986-aff2-a1eb4bfbbc2b",
   "metadata": {},
   "source": [
    "### Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60681939-604c-469c-8037-8abaec8302db",
   "metadata": {},
   "source": [
    "Boosting is a machine learning ensemble technique that involves sequentially training multiple weak models, with each model focusing on the instances that were misclassified by the previous models. The idea behind boosting is to combine the strengths of multiple weak models to create a strong model that can make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64491b-9b24-4fdc-bbd2-a44d941f27e5",
   "metadata": {},
   "source": [
    "### Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6a2814-5731-4855-a58e-71a0c1f657b1",
   "metadata": {},
   "source": [
    "1. Improved accuracy: Ensemble learning can improve the accuracy of a model by combining the predictions of multiple models, each of which may have different strengths and weaknesses.\n",
    "\n",
    "2. Increased stability: Ensemble learning can help to reduce the variance of a model by combining the predictions of multiple models trained on different subsets of the data. This can help to make the model more robust and less prone to overfitting.\n",
    "\n",
    "3. Generalization: Ensemble learning can help to improve the generalization of a model by reducing the impact of noise or outliers in the data. By combining the predictions of multiple models, the ensemble can learn to ignore noise and focus on the underlying patterns in the data.\n",
    "\n",
    "4. Model selection: Ensemble learning can be used to combine the predictions of multiple models trained with different hyperparameters or architectures. This can help to identify the best model for a given task and improve the overall performance of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b68c63-1648-4087-bd2f-644efc82d905",
   "metadata": {},
   "source": [
    "### Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfe2cb-727f-4b6b-bec8-7074fa62e737",
   "metadata": {},
   "source": [
    "Ensemble learning can often provide better performance than individual models, but this is not always the case. The performance of an ensemble model depends on various factors, such as the diversity and quality of the individual models, the size and quality of the training data, the complexity of the problem, and the performance metric used to evaluate the model.\n",
    "\n",
    "In some cases, an individual model may be sufficient to achieve good performance, especially if the data is simple and the model is well-tuned. However, in many cases, an ensemble of models can provide better performance by reducing overfitting, increasing generalization, and improving the robustness of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3951e07-990a-4ec6-8d05-500bf008d71f",
   "metadata": {},
   "source": [
    "### Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cd6a3-9476-4259-9c91-90be2ad7baed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f7cfe56-94bc-4d4e-94d9-95aed30b908f",
   "metadata": {},
   "source": [
    "### Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91facf3-d982-4d14-89e6-aaff582ac8b0",
   "metadata": {},
   "source": [
    "To obtain the general statistic when our dataset is small we use Bootstrapping where we randomly sample with replacement new bootstrapped samples of the same size as original sample, evaluate the metric and generalize the results from a dataset consisting of artificially created data\n",
    "\n",
    "The steps are\n",
    " 1. Make bootstrapped data\n",
    " 2. Calculate the metric\n",
    " 3. Keep track of the metric\n",
    " 4. Repeat it multiple times\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb32db0-039b-4037-a1a2-a230ada03178",
   "metadata": {},
   "source": [
    "### Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31d2a2-a776-46fd-a777-5759eb5e188a",
   "metadata": {},
   "source": [
    "1. Take a random sample of size n=50 from the population data.\n",
    "\n",
    "2. Create a large number of bootstrap samples by randomly sampling with replacement from the sample data, each of size n=50. For example, create 10,000 bootstrap samples.\n",
    "\n",
    "3. Calculate the mean height for each bootstrap sample.\n",
    "\n",
    "4. Calculate the standard error of the mean by taking the standard deviation of the bootstrap sample means. The standard error is given by:\n",
    "\n",
    "5. standard error = standard deviation / sqrt(n) = 2 / sqrt(50) = 0.283\n",
    "\n",
    "6. Calculate the 95% confidence interval by taking the 2.5th and 97.5th percentile values of the bootstrap sample means. For example:\n",
    "\n",
    "7. lower bound = mean(bootstrap sample means) - 1.96 * standard error\n",
    "= 15 - 1.96 * 0.283\n",
    "= 14.44 meters\n",
    "\n",
    "8. upper bound = mean(bootstrap sample means) + 1.96 * standard error\n",
    "= 15 + 1.96 * 0.283\n",
    "= 15.56 meters\n",
    "\n",
    "9. Therefore, the 95% confidence interval for the population mean height of trees is [14.44 meters, 15.56 meters]. This means that we can be 95% confident that the true population mean height falls within this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7721387-e63f-4eac-b684-8d6aa12714e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

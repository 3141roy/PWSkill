{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f208bb-c1db-4eda-9d75-24a44497be42",
   "metadata": {},
   "source": [
    "#### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distancemetric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c9ca2-9b64-40b8-83f2-9770bdb59d43",
   "metadata": {},
   "source": [
    "Euclidean distance, also known as the straight-line distance or L2 norm, calculates the distance as the square root of the sum of the squared differences between corresponding coordinates. In other words, it measures the length of the straight line connecting two points.\n",
    "\n",
    "Manhattan distance, also known as the city block distance or L1 norm, calculates the distance as the sum of the absolute differences between corresponding coordinates. It measures the distance as the total number of blocks traveled horizontally and vertically to reach from one point to another, similar to navigating through a city block grid.\n",
    "\n",
    "Factors affecting choice of distance metric are\n",
    "\n",
    "1. Sensitivity to feature scales: Euclidean distance takes into account both the magnitude and direction of the differences between coordinates. As a result, it can be sensitive to the scale of the features. If the scales of different features vary significantly, the features with larger magnitudes may dominate the distance calculation. On the other hand, Manhattan distance, being based on absolute differences, is not as sensitive to feature scales. It treats all dimensions equally, making it more suitable when feature scales are not comparable.\n",
    "\n",
    "2. Influence of outliers: Euclidean distance gives equal weight to all dimensions, and as a result, it can be heavily influenced by outliers. A single outlier with a large magnitude can significantly affect the Euclidean distance between two points. In contrast, Manhattan distance is less sensitive to outliers since it only considers the absolute differences between coordinates. It measures the distance based on the number of blocks traveled, and outliers do not distort the overall path as much.\n",
    "\n",
    "3. Spatial characteristics: The choice of distance metric can also depend on the spatial characteristics of the data. Euclidean distance is well-suited for continuous and smooth data, where the magnitude and direction of differences are meaningful. On the other hand, Manhattan distance is often preferred for categorical or binary data, where absolute differences provide a meaningful measure of dissimilarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1cb97-cf35-4e4e-b425-ca40a5f4225f",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc4620-1ad0-4239-8456-5ca6a6ec9ef7",
   "metadata": {},
   "source": [
    "We can use methods like GridSearchCV to find the optimum value of k for the KNN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36cf49b-f03b-4921-89a0-ac1b3c62fae0",
   "metadata": {},
   "source": [
    "#### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b8747-d8e9-4343-aad8-01db19632860",
   "metadata": {},
   "source": [
    "Euclidean distance: The Euclidean distance is the most commonly used distance metric in KNN. It measures the straight-line distance between two points in a multi-dimensional space. It is suitable for continuous data and situations where the magnitude and direction of differences are meaningful. Some situations where Euclidean distance might be preferred include:\n",
    "\n",
    "- Continuous and smooth data: Euclidean distance works well for datasets with continuous features, such as sensor measurements or physical attributes.\n",
    "- Euclidean geometry: If the problem domain follows Euclidean geometry, where straight-line distances are more meaningful, Euclidean distance is a natural choice.\n",
    "- Feature scales are comparable: Euclidean distance assumes that all features contribute equally to the distance calculation. Therefore, it is advisable to normalize or scale the features when using Euclidean distance to avoid biased results due to differences in feature scales.\n",
    "\n",
    "\n",
    "Manhattan distance: The Manhattan distance, also known as the city block distance or L1 norm, measures the distance as the sum of absolute differences between corresponding coordinates. It is suitable for categorical or binary data and situations where only the difference in coordinates matters. Some situations where Manhattan distance might be preferred include:\n",
    "\n",
    "- Categorical data: When dealing with categorical or binary features, where the absolute differences between categories are meaningful but the magnitude is not, Manhattan distance can be more appropriate.\n",
    "- Unequal feature scales: Manhattan distance is not sensitive to feature scales. If the scales of different features vary significantly, using Manhattan distance can avoid dominance by features with larger magnitudes.\n",
    "- Robustness to outliers: Manhattan distance is less sensitive to outliers since it calculates distances based on the number of blocks traveled. It can be a better choice when dealing with datasets that contain outliers or when outliers should have less influence on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c9cd3-98ee-4c7b-b468-c77c89c14611",
   "metadata": {},
   "source": [
    "#### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd15b6-b9ae-4b67-a40d-8e72c6cbd47c",
   "metadata": {},
   "source": [
    "1. Number of neighbors (K): This hyperparameter determines the number of nearest neighbors to consider when making predictions. Choosing the right value for K is crucial, as it can affect the model's bias-variance tradeoff. A smaller K can lead to more flexible decision boundaries but may increase the model's sensitivity to noise and outliers. On the other hand, a larger K can smooth out the decision boundaries but may oversimplify the model. The optimal value of K depends on the dataset and problem at hand.\n",
    "\n",
    "2. Distance metric: The choice of distance metric, such as Euclidean distance or Manhattan distance, was discussed earlier. The distance metric affects how similarity or dissimilarity between data points is calculated. It is essential to select a distance metric that aligns well with the characteristics of the data. Experimentation and evaluation on the specific dataset can help determine the most suitable distance metric.\n",
    "\n",
    "3. Weighting scheme: In KNN, the weighting scheme determines the contribution of each neighbor to the final prediction. The two common weighting schemes are uniform and distance-based weighting. In uniform weighting, each neighbor has an equal vote in the prediction. In distance-based weighting, closer neighbors have more influence. Distance-based weighting can be useful when closer neighbors are expected to provide more reliable information, especially in cases where neighbors farther away may be less relevant.\n",
    "\n",
    "We can use GridSearchCV and CrossValidation to get the best set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05febecc-2707-43b4-b1db-ca82c38e0b1f",
   "metadata": {},
   "source": [
    "#### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bde5ef-25c0-4235-a401-a0455ffada1c",
   "metadata": {},
   "source": [
    "KNN is affected by the curse of dimensionality, where the distance between points becomes less informative as the number of dimensions increases. With a fixed training set size, as the number of dimensions grows, the density of the training points in the space decreases, leading to a higher likelihood of misclassification or inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc104bd-5011-423f-8c66-bb7e8a9ac073",
   "metadata": {},
   "source": [
    "#### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f01d-b800-4404-98cc-51c2a1c057c2",
   "metadata": {},
   "source": [
    "\n",
    "While K-nearest neighbors (KNN) is a simple and intuitive algorithm, it has some potential drawbacks that can affect its performance. Here are a few drawbacks of using KNN as a classifier or regressor and some strategies to overcome them:\n",
    "\n",
    "1. Computational complexity: The computational cost of KNN can be high, especially as the size of the training set and the number of dimensions increase. For large datasets, the algorithm needs to calculate distances between the query point and all training points, which can be time-consuming. To address this, you can use techniques like dimensionality reduction to reduce the number of features or employ approximate nearest neighbor algorithms that trade off accuracy for computational efficiency, such as k-d trees or locality-sensitive hashing.\n",
    "\n",
    "2. Sensitivity to feature scales: KNN calculates distances between data points, and if the features have different scales, those with larger magnitudes can dominate the distance calculations. To overcome this, it is advisable to normalize or scale the features so that they have similar ranges. Techniques like min-max scaling or standardization can be applied to ensure all features contribute proportionally to the distance metric.\n",
    "\n",
    "3. Determining the optimal value of K: The choice of the number of neighbors (K) in KNN is crucial, and selecting an inappropriate value can impact the model's performance. If K is too small, the model may be sensitive to noise and outliers, leading to overfitting. On the other hand, if K is too large, the model may introduce bias and oversmooth the decision boundaries. Techniques like cross-validation, grid search, or the elbow method can help determine the optimal value of K for a given dataset.\n",
    "\n",
    "4. Imbalanced class distribution: KNN can be sensitive to imbalanced class distributions since it relies on majority voting. In datasets where the classes are imbalanced, the majority class can dominate the predictions. To mitigate this issue, you can use techniques such as oversampling the minority class, undersampling the majority class, or applying class weighting to give more importance to the minority class during the prediction phase.\n",
    "\n",
    "5. Curse of dimensionality: KNN can suffer from the curse of dimensionality, where the performance deteriorates as the number of dimensions increases. In high-dimensional spaces, the distance between points becomes less informative, and the algorithm may struggle to find meaningful neighbors. Dimensionality reduction techniques, such as PCA or feature selection methods, can help alleviate this issue by reducing the number of dimensions while preserving the most relevant information.\n",
    "\n",
    "6. Outliers and noisy data: KNN can be sensitive to outliers and noisy data points since it relies on the distances between data points. Outliers can disproportionately influence the classification or regression results. One approach to handle outliers is to use robust distance metrics, such as the Mahalanobis distance, that are less affected by extreme values. Alternatively, you can employ outlier detection techniques to identify and handle outliers before applying the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1786275-b280-41e5-8d76-92e456c6d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

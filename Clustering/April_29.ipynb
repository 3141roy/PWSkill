{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c108bc0-d12f-4519-a47f-ddf213bada09",
   "metadata": {},
   "source": [
    "#### Q1. Explain the basic concept of clustering and give examples of applications where clustering is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741deb5-d4ce-4ef9-82ec-21f6f7952b01",
   "metadata": {},
   "source": [
    "Clustering is a fundamental concept in machine learning and data analysis that involves grouping similar data points together based on their intrinsic characteristics or similarities. The goal is to discover patterns, structures, or hidden relationships within the data without any prior knowledge of the groupings.\n",
    "    It is a part of unsupervised machine learning technique to make predictions based on a large available group of data\n",
    "    \n",
    "Some real world uses include\n",
    "\n",
    "    1. Customer Segmentation: Clustering helps in segmenting customers based on their purchasing behavior, preferences, or demographics. This information can be used for targeted marketing, personalized recommendations, or customer relationship management.\n",
    "\n",
    "    2. Image and Object Recognition: Clustering techniques can assist in grouping similar images or objects together, aiding in image categorization, object recognition, and image retrieval systems.\n",
    "\n",
    "    3. Document Clustering and Topic Modeling: Clustering algorithms can group similar documents together, enabling document organization, topic modeling, and information retrieval in large text corpora. It helps in tasks like document categorization, sentiment analysis, and text mining.\n",
    "\n",
    "    4. Anomaly Detection: Clustering can identify unusual or anomalous data points that deviate from normal patterns. It is useful in fraud detection, network intrusion detection, and identifying outliers in datasets.\n",
    "\n",
    "    5. Genomic Analysis: Clustering techniques are employed in genomics to group genes or DNA sequences with similar expression patterns or functional characteristics. It aids in identifying gene clusters related to specific diseases or biological processes.\n",
    "\n",
    "    6. Social Network Analysis: Clustering is applied to analyze social networks, identify communities, and detect influential individuals or groups. It helps in understanding social interactions, recommendation systems, and viral marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d58b8-1c2f-48d0-ac94-388efbf6fdf6",
   "metadata": {},
   "source": [
    "#### Q2. What is DBSCAN and how does it differ from other clustering algorithms such as k-means and hierarchical clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c518e4-376e-4856-9744-e6222a0c1c28",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups data points based on their density in the feature space. Unlike k-means and hierarchical clustering, DBSCAN does not require specifying the number of clusters in advance and can discover clusters of arbitrary shapes.\n",
    "\n",
    "Here are the key characteristics that differentiate DBSCAN from other clustering algorithms:\n",
    "\n",
    "    1. Density-Based Clustering: DBSCAN defines clusters based on the density of data points. It identifies dense regions separated by sparser regions. Points in high-density regions are considered core points, while points in low-density regions are considered noise or outliers.\n",
    "\n",
    "    2. Discovery of Arbitrary-Shaped Clusters: DBSCAN can discover clusters with complex shapes, including clusters that are non-linear or have irregular boundaries. It can handle clusters of varying densities, sizes, and shapes without making any assumptions about their geometry.\n",
    "\n",
    "    3. No Prespecified Number of Clusters: DBSCAN does not require specifying the number of clusters in advance. It automatically determines the number of clusters based on the density and connectivity of the data points.\n",
    "\n",
    "    4. Parameterized by Distance and Density: DBSCAN relies on two key parameters: \"epsilon\" (ε), which defines the radius around a point to determine its neighborhood, and \"minPts,\" which specifies the minimum number of points required to form a dense region or cluster. These parameters influence the density threshold for cluster formation.\n",
    "\n",
    "    5. Handling of Noise and Outliers: DBSCAN can identify and classify points that do not belong to any cluster as noise or outliers. These points are typically located in regions of low density and are not associated with any dense cluster structure.\n",
    "\n",
    "In contrast, k-means is a __partition-based centroid clustering algorithm__ that aims to divide data points into a pre-defined number of clusters, optimizing the sum of squared distances between data points and their assigned cluster centroids. It assumes spherical clusters of similar sizes.\n",
    "\n",
    "Hierarchical clustering, on the other hand, __builds a hierarchy of clusters__ by iteratively merging or splitting clusters based on certain distance metrics. It can be agglomerative (bottom-up) or divisive (top-down) and creates a dendrogram representing the clustering structure. Hierarchical clustering is sensitive to the choice of distance metrics and can handle various data types.\n",
    "\n",
    "While k-means and hierarchical clustering have their strengths, DBSCAN offers distinct advantages when dealing with complex data distributions, unknown cluster counts, and noise handling. DBSCAN's ability to discover clusters of arbitrary shapes and its parameterization by density make it a powerful clustering algorithm in various real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c6ccc-0fd4-4df6-930f-a4f3eb976b6e",
   "metadata": {},
   "source": [
    "#### Q3. How do you determine the optimal values for the epsilon and minimum points parameters in DBSCAN clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4668ad-5cff-40b1-89ff-93dd14ff58a4",
   "metadata": {},
   "source": [
    "Determining the optimal values for the epsilon (ε) and minimum points parameters in DBSCAN clustering can be approached through various methods. Here are a few commonly used techniques:\n",
    "\n",
    "    Domain Knowledge: Domain knowledge and understanding of the dataset can provide valuable insights for selecting appropriate parameter values. Consider the characteristics of the data, the expected density of the clusters, and the scale of the features. Adjust the parameters based on your understanding of the problem and the specific requirements of the application.\n",
    "\n",
    "    Visual Inspection: One way to determine suitable parameter values is to visualize the results of DBSCAN with different parameter combinations. Plot the clusters and examine the clustering output for various parameter values. Look for stable and meaningful cluster structures. Visual inspection can help identify parameter values that produce clusters consistent with domain knowledge or desired clustering results.\n",
    "\n",
    "    k-Distance Graph: The k-distance graph can assist in choosing epsilon (ε). For each point, calculate the distance to its kth nearest neighbor, where k is determined based on the data and problem context. Plot these distances in ascending order. Look for the knee point, which represents a significant change in the distance values. The distance corresponding to the knee point can serve as a reasonable estimate for epsilon.\n",
    "\n",
    "    Reachability Distance Plot: Generate a reachability distance plot, which illustrates the distances between points in the dataset. Plot the distances in descending order. Observe the plot to identify a suitable range of distances where points are relatively close to each other. This range can help guide the selection of the epsilon parameter.\n",
    "\n",
    "    Silhouette Score: The silhouette score measures the compactness and separation of clusters. Calculate the silhouette score for different combinations of parameter values. Higher silhouette scores indicate better-defined clusters. Opt for parameter values that maximize the silhouette score.\n",
    "\n",
    "    Grid Search or Parameter Tuning: Automated approaches, such as grid search or parameter tuning techniques, can systematically explore a range of parameter values and evaluate their impact on clustering quality. This involves evaluating clustering performance metrics, such as silhouette score or clustering stability, for different combinations of epsilon and minimum points values. The optimal parameter values are determined based on the best-performing combination.\n",
    "\n",
    "It's important to note that the selection of epsilon and minimum points parameters in DBSCAN may require some experimentation and fine-tuning. It is often a trade-off between capturing the desired density-based clusters, avoiding over-segmentation, and accounting for noise. The choice of parameters depends on the specific characteristics of the data, the problem at hand, and the desired clustering outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265985cd-ccd9-4c5d-84f5-3fc9c2e55620",
   "metadata": {},
   "source": [
    "#### Q4. How does DBSCAN clustering handle outliers in a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d40186c-226c-4bcb-adad-5a4e1cdced3c",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering handles outliers in a dataset as part of its inherent functionality. Here's how DBSCAN handles outliers:\n",
    "\n",
    "1. __Core Points__: DBSCAN identifies core points as data points that have a sufficient number of neighboring points within a specified distance (epsilon, ε). These core points are considered to be part of a dense region or cluster.\n",
    "\n",
    "2. __Border Points__: Border points in DBSCAN have fewer neighboring points within ε compared to core points but are still reachable from a core point. These points are on the outskirts of a cluster and may have a lower density.\n",
    "\n",
    "3. __Noise Points/Outliers__: Data points that are neither core points nor reachable from any core points are considered noise points or outliers. These points do not belong to any specific cluster or dense region and are often isolated or located in low-density areas.\n",
    "\n",
    "By design, DBSCAN explicitly identifies and distinguishes outliers from the clustered data points. It does not assign noise points to any specific cluster and treats them separately.\n",
    "\n",
    "The handling of outliers in DBSCAN has some advantages:\n",
    "\n",
    "1. __Noise Removal__: DBSCAN can effectively filter out noise or irrelevant data points that do not conform to any cluster structure. This is particularly useful in datasets where outliers can significantly affect the clustering results or when the presence of noise points is undesirable.\n",
    "\n",
    "2. __Robustness to Outliers__: DBSCAN's density-based approach allows it to be robust to outliers. Outliers that are sufficiently distant from any cluster are considered noise points, as they fail to meet the density requirements for clustering. This reduces the impact of outliers on the overall clustering outcome.\n",
    "\n",
    "3. __Flexibility in Density__: DBSCAN's ability to handle varying density regions allows it to identify clusters of different sizes and shapes, while simultaneously identifying and labeling outlier points. This flexibility enables the discovery of clusters in datasets with irregular or complex density distributions.\n",
    "\n",
    "It's important to note that the proper determination of the epsilon (ε) parameter in DBSCAN is crucial for effectively identifying outliers. A larger epsilon value may result in more points being labeled as noise, while a smaller epsilon value may cause some outliers to be misclassified as belonging to a cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b05ecf-420e-4e1e-996b-3e868948616c",
   "metadata": {},
   "source": [
    "#### Q5. How does DBSCAN clustering differ from k-means clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef7879-7557-46bb-9036-b0a2ec8b6420",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and k-means clustering are two distinct clustering algorithms with different approaches and characteristics. Here are the key differences between DBSCAN and k-means clustering:\n",
    "\n",
    "__Clustering Approach__:\n",
    "\n",
    "    DBSCAN: DBSCAN is a density-based clustering algorithm that groups data points based on their density in the feature space. It identifies dense regions separated by sparser regions and does not require the number of clusters to be specified in advance.\n",
    "    \n",
    "    k-means: k-means is a centroid-based clustering algorithm that partitions data points into a pre-defined number of clusters. It aims to minimize the sum of squared distances between data points and their assigned cluster centroids. It assumes spherical clusters and requires the number of clusters to be known or specified.\n",
    "\n",
    "__Cluster Shape and Size__:\n",
    "\n",
    "    DBSCAN: DBSCAN can discover clusters of arbitrary shapes and sizes. It can handle clusters that are non-linear, have irregular boundaries, and vary in density. DBSCAN is particularly effective at identifying clusters with complex shapes or clusters of varying densities.\n",
    "\n",
    "    k-means: k-means assumes clusters that are spherical and of similar sizes. It works well with well-separated and compact clusters but may struggle with clusters that have irregular shapes or varying sizes.\n",
    "    \n",
    "__Handling of Outliers__:\n",
    "\n",
    "    DBSCAN: DBSCAN has a built-in mechanism to handle outliers. It classifies data points that do not belong to any cluster as noise or outliers. Outliers are points that are located in regions of low density or are isolated from dense clusters.\n",
    "\n",
    "    k-means: k-means does not explicitly handle outliers. Outliers can have a significant impact on the centroid calculation and cluster assignment in k-means. They can distort the position and size of the clusters, leading to suboptimal results.\n",
    "\n",
    "__Parameter Sensitivity__:\n",
    "\n",
    "    DBSCAN: DBSCAN has two key parameters: epsilon (ε), which defines the radius for determining a point's neighborhood, and minPts, which specifies the minimum number of points required to form a dense region. The performance of DBSCAN can be sensitive to the choice of these parameters.\n",
    "\n",
    "    k-means: k-means is sensitive to the initial choice of cluster centroids, which can affect the final clustering results. The algorithm is also sensitive to outliers, as they can pull the centroids away from the true cluster centers.\n",
    "\n",
    "__Clustering Flexibility__:\n",
    "\n",
    "    DBSCAN: DBSCAN is more flexible in discovering clusters of varying densities and shapes. It can handle datasets with irregular or complex structures and is less influenced by the number of clusters.\n",
    "\n",
    "    k-means: k-means requires the number of clusters to be specified in advance, making it less flexible when the true number of clusters is unknown. It assumes that clusters have similar sizes and shapes, and it may struggle with datasets that violate these assumptions.\n",
    "\n",
    "In summary, DBSCAN and k-means clustering differ in their approach to clustering, handling of outliers, flexibility in cluster shape and size, and sensitivity to parameters. DBSCAN is suitable for datasets with complex structures and varying densities, while k-means is often used for well-separated, compact, and spherical clusters when the number of clusters is known in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d168d-37ef-4836-889e-ac56b5fd2349",
   "metadata": {},
   "source": [
    "#### Q6. Can DBSCAN clustering be applied to datasets with high dimensional feature spaces? If so, what are some potential challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f91cdd-79bb-43e2-94d3-fbcb8a6775e7",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can be applied to datasets with high-dimensional feature spaces. However, clustering high-dimensional data presents some challenges. Here are some potential challenges when applying DBSCAN clustering to high-dimensional datasets:\n",
    "\n",
    "    Curse of Dimensionality: High-dimensional spaces suffer from the curse of dimensionality. As the number of dimensions increases, the data becomes more sparse, and the distance between points tends to become less meaningful. This can affect the effectiveness of distance-based similarity measures used in DBSCAN, leading to difficulties in determining appropriate parameter values.\n",
    "\n",
    "    Increased Distance Measures: In high-dimensional spaces, the notion of distance becomes less discriminative. The distances between data points tend to be more evenly distributed, and the differences in distances become less informative. Traditional distance metrics may lose their effectiveness in capturing meaningful similarities or dissimilarities between points.\n",
    "\n",
    "    Density Estimation: Estimating density accurately in high-dimensional spaces becomes challenging due to the sparsity of data. Determining the appropriate value for the epsilon (ε) parameter in DBSCAN becomes more difficult, as the concept of a neighborhood becomes less well-defined in high-dimensional feature spaces.\n",
    "\n",
    "    Dimensional Irrelevance: High-dimensional data often contains dimensions that are irrelevant or noisy. These irrelevant dimensions can dominate the distance calculations and hinder the clustering process. Feature selection or dimensionality reduction techniques may be necessary to mitigate the impact of irrelevant dimensions and improve clustering results.\n",
    "\n",
    "    Visualization and Interpretability: Visualizing high-dimensional data is inherently challenging due to limitations in human perception. It becomes difficult to visualize and interpret the clustering results in their original feature space. Dimensionality reduction techniques, such as t-SNE or PCA, can be used to project the data into a lower-dimensional space for visualization purposes.\n",
    "\n",
    "To address these challenges, it is recommended to consider some strategies when applying DBSCAN to high-dimensional data:\n",
    "\n",
    "    Feature Selection: Identify and select relevant features that contribute significantly to the clustering structure. Removing irrelevant or noisy dimensions can improve the clustering performance.\n",
    "\n",
    "    Dimensionality Reduction: Apply dimensionality reduction techniques, such as Principal Component Analysis (PCA) or t-SNE, to reduce the dimensionality of the data while preserving the important clustering structure. This can aid in visualization, interpretability, and potentially improve clustering results.\n",
    "\n",
    "    Customized Distance Metrics: Develop customized distance metrics or similarity measures that are more suitable for the specific characteristics of the high-dimensional data. These metrics can consider the relevance and importance of different dimensions or incorporate domain-specific knowledge.\n",
    "\n",
    "    Preprocessing Techniques: Apply appropriate preprocessing techniques, such as normalization or scaling, to handle the differences in the scales or variances of the features in high-dimensional data.\n",
    "\n",
    "    Parameter Tuning: Parameter tuning becomes more crucial in high-dimensional DBSCAN. Experiment with different parameter values for epsilon (ε) and minPts to find the best configuration that captures meaningful clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08863e1-c070-47c9-877e-bb9e01fd138f",
   "metadata": {},
   "source": [
    "#### Q7. How does DBSCAN clustering handle clusters with varying densities?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b5f2f-5980-4b1d-b7a7-f3aeb17eb9f5",
   "metadata": {},
   "source": [
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering is particularly effective at handling clusters with varying densities. Here's how DBSCAN handles clusters with different densities:\n",
    "\n",
    "    Core Points: DBSCAN identifies core points as data points that have a sufficient number of neighboring points within a specified distance (epsilon, ε). These core points are considered part of dense regions or clusters.\n",
    "\n",
    "    Density-Reachability: DBSCAN defines density-reachability between points to determine cluster membership. A point is considered density-reachable from another point if there is a path of core points leading from one point to another, with each consecutive point within ε distance of its neighbor. This allows DBSCAN to capture dense areas within the dataset.\n",
    "\n",
    "    Varying Epsilon (ε) Parameter: DBSCAN's flexibility lies in its parameterization. By adjusting the epsilon parameter, which defines the radius for determining a point's neighborhood, DBSCAN can handle clusters with varying densities. For dense regions, a smaller epsilon value can be used to identify nearby points, while a larger epsilon value can be used for sparse regions.\n",
    "\n",
    "    Direct Connectivity vs. Indirect Connectivity: DBSCAN considers both direct connectivity and indirect connectivity to establish clusters. Direct connectivity is established when two points are within ε distance of each other. Indirect connectivity occurs when two points are not directly connected but can be reached by a path of core points within ε distance. This allows DBSCAN to identify clusters of varying densities, including clusters connected by a few sparse regions.\n",
    "\n",
    "    Noise Points: In DBSCAN, points that do not belong to any cluster are classified as noise or outliers. These points are typically located in regions of low density and are not associated with any dense cluster structure.\n",
    "\n",
    "By leveraging the concept of density and connectivity, DBSCAN can effectively identify clusters with varying densities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b25f4-e0b8-4866-8add-25804b0aa633",
   "metadata": {},
   "source": [
    "#### Q8. What are some common evaluation metrics used to assess the quality of DBSCAN clustering results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249f4da-8b9c-4056-94f3-ebcbe2d7f236",
   "metadata": {},
   "source": [
    "__The Silhouette coefficient__ measures the compactness and separation of clusters. It considers both intra-cluster cohesion and inter-cluster separation. The coefficient ranges from -1 to 1, where values closer to 1 indicate well-separated and compact clusters, values close to 0 indicate overlapping clusters, and negative values indicate misclassified or poorly separated points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2fb70-1ebf-4d53-8b0b-6a1dce648e09",
   "metadata": {},
   "source": [
    "#### Q9. Can DBSCAN clustering be used for semi-supervised learning tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6052d-c4a4-4f1e-b31f-e317d963828c",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering is primarily an unsupervised learning algorithm that does not rely on any labeled data. However, DBSCAN can be utilized in semi-supervised learning tasks with the help of additional information. Here's how DBSCAN can be used in semi-supervised learning:\n",
    "\n",
    "    Initial Clustering: DBSCAN can be applied to the unlabeled data to generate an initial clustering. This clustering assigns labels to the data points based on their density and connectivity, identifying dense regions as clusters and labeling outliers as noise points.\n",
    "\n",
    "    Seed Points: In semi-supervised learning, a subset of data points may be labeled, either manually or through another process. These labeled data points can serve as seed points or anchors within the clusters.\n",
    "\n",
    "    Label Propagation: The labels from the seed points can be propagated to neighboring unlabeled points within the same cluster. Since DBSCAN captures the density and connectivity of the data, neighboring points within the same cluster are likely to have similar characteristics. By propagating labels from seed points, the cluster labels can be extended to the unlabeled data points within the same cluster.\n",
    "\n",
    "    Iterative Process: The label propagation step can be performed iteratively, updating and refining the labels of the unlabeled data points based on their neighbors' labels within the same cluster. This iterative process helps propagate labels through the clusters and improve the labeling accuracy.\n",
    "\n",
    "By combining the initial clustering from DBSCAN with the labeled data, semi-supervised learning algorithms can leverage the density-based structure of the data to propagate labels and make predictions for the unlabeled data points. This approach can be particularly useful when the labeled data is limited, and the data exhibits dense regions or cluster structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27b976-2d11-4d8e-b2a6-56ab14263cc1",
   "metadata": {},
   "source": [
    "#### Q10. How does DBSCAN clustering handle datasets with noise or missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3aa3e-9bf2-4ca9-9b0f-e9449cf2baa7",
   "metadata": {},
   "source": [
    "Noise Handling:\n",
    "\n",
    "DBSCAN identifies noise points as data points that do not belong to any cluster. These points are typically located in regions of low density or isolated from dense clusters. DBSCAN does not assign these points to any cluster, treating them as outliers or noise.\n",
    "Missing Values Handling:\n",
    "\n",
    "Handling missing values in DBSCAN can be challenging since DBSCAN relies on distance calculations. Missing values in the data can disrupt the distance computations and affect the clustering process.\n",
    "\n",
    "   1.   One approach is to impute the missing values with appropriate techniques before applying DBSCAN. Imputation methods such as mean imputation, median imputation, or more advanced techniques like k-nearest neighbors (KNN) imputation can be used to fill in the missing values based on the available data.\n",
    "    \n",
    "   2.  Another approach is to assign a special value or a separate category to represent missing values and consider them as a distinct category in the distance calculations. This approach allows DBSCAN to handle missing values explicitly but may require careful definition of the distance metric to account for the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cc7802-658e-4653-9141-d6e921332e74",
   "metadata": {},
   "source": [
    "#### Q11. Implement the DBSCAN algorithm using a python programming language, and apply it to a sample dataset. Discuss the clustering results and interpret the meaning of the obtained clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d214e9c-4a05-4e3a-9802-ed58394d26b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters is :  2\n",
      "Cluster 1 : [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]]\n",
      "Cluster 2 : [[7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "dbscan = DBSCAN(eps = 0.5, min_samples = 5)\n",
    "dbscan.fit(X_scaled)\n",
    "\n",
    "labels = dbscan.labels_\n",
    "noise_points = X[labels ==-1]\n",
    "\n",
    "num = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"Number of clusters is : \",num)\n",
    "\n",
    "for i in range(num):\n",
    "    cluster_points = X[labels == i]\n",
    "    print(\"Cluster\", i+1, \":\", cluster_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c13c90-bea2-4dab-b827-b3bff121ec17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

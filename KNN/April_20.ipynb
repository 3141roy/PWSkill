{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c221349-914f-466e-935c-5dbb8657c788",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158a7a8-5eac-4d9f-806f-cbf7d1f8b89f",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric classification algorithm used for both regression and classification problems in machine learning. It is a supervised learning algorithm that works by finding the k number of nearest training examples (neighbors) to the given query data point, and then uses their labels to predict the label of the query data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c903bb-8a0c-4cf3-a061-df4773d5b86e",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee3f5d-65cf-41a4-8cf3-1b99c9e46396",
   "metadata": {},
   "source": [
    "Choosing the right value of K in the KNN algorithm is an important task, as it can significantly affect the performance of the model. A small value of K can lead to overfitting, while a large value of K can lead to underfitting.\n",
    "\n",
    "Having domain experience plays an importangt role too, else we can use Best fit methods like Grid Search CV gto find the optimum value.\n",
    "The choice of distance metric can also affect the optimal value of K. For example, if Euclidean distance is used, a smaller value of K may be more appropriate, while for Manhattan distance, a larger value of K may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7222be2e-81a8-4b93-89dc-b3f12173d194",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaf2f9-9e9d-49a8-a5ff-3852a811f51d",
   "metadata": {},
   "source": [
    "KNN classifier: In KNN classification, the output is a class label. The algorithm classifies a new data point based on the majority class label of its k-nearest neighbors. For example, if the k-nearest neighbors of a new data point belong to classes A, B, and A, the algorithm will classify the new data point as class A.\n",
    "\n",
    "KNN regressor: In KNN regression, the output is a continuous value. The algorithm predicts the output of a new data point by taking the mean or median value of the k-nearest neighbors' output. For example, if the k-nearest neighbors of a new data point have outputs 10, 15, and 12, the algorithm will predict the output of the new data point as the mean or median value of these outputs, which is 12.33 or 12, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a080a3-2062-4a44-b24e-d9c259bd4bfe",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785a765-bce5-41de-9730-b27e4f0d4854",
   "metadata": {},
   "source": [
    "1. Classification accuracy: Classification accuracy is the proportion of correctly classified data points to the total number of data points. It is calculated by dividing the number of correct predictions by the total number of predictions.\n",
    "\n",
    "2. Confusion matrix: A confusion matrix is a table that summarizes the predicted and actual class labels for a classification problem. It shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "3. Precision and recall: Precision is the ratio of true positives to the sum of true positives and false positives. Recall is the ratio of true positives to the sum of true positives and false negatives. They are useful metrics when the classes are imbalanced.\n",
    "\n",
    "4. F1-score: F1-score is the harmonic mean of precision and recall. It is a good metric to use when the classes are imbalanced, and we want to balance between precision and recall.\n",
    "\n",
    "5. Mean squared error: Mean squared error is a commonly used metric to evaluate regression models. It is the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "6. R-squared: R-squared is a metric used to evaluate regression models. It measures the proportion of the variance in the target variable that is explained by the independent variables. A higher value of R-squared indicates a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca209f5-ca2e-4f33-95bd-d4fcd698dcca",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a946b7-043e-4af3-bd13-4813aba53b4f",
   "metadata": {},
   "source": [
    " As the number of dimensions increases, the distance between data points becomes less meaningful. In high-dimensional space, most of the points are equidistant from each other, making it difficult to find the k-nearest neighbors of a given data point. This can lead to overfitting and poor performance of the KNN algorithm.\n",
    "\n",
    "Additionally, as the number of dimensions increases, the amount of training data required to achieve a good performance also increases exponentially. This is because the number of possible combinations of features increases exponentially with the number of dimensions. Therefore, in high-dimensional space, the amount of data required to achieve good performance becomes impractically large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37faee-88f4-47c9-9b75-8eb14e885dab",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd3745-3a3f-4f5b-af2f-f7f2e53b3e45",
   "metadata": {},
   "source": [
    "1. Deletion: One straightforward approach is to delete the data points that have missing values. This approach is simple but can result in a loss of useful information.\n",
    "\n",
    "2. Imputation: Imputation is the process of filling in missing values with estimated values. There are several imputation methods that can be used in KNN, such as mean imputation, mode imputation, and regression imputation. Mean imputation replaces missing values with the mean of the non-missing values of that feature. Mode imputation replaces missing values with the mode of the non-missing values of that feature. Regression imputation uses a regression model to predict the missing values based on the non-missing values of other features.\n",
    "\n",
    "3. KNN imputation: KNN imputation is a more advanced imputation method that uses the KNN algorithm to fill in missing values. The algorithm identifies the k-nearest neighbors of a data point with missing values and fills in the missing values with the mean or median value of the corresponding features of the k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae6ac6-903c-41b0-895e-7ce1241073a2",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a5abb-6ab3-4707-bd00-993fd6b0832e",
   "metadata": {},
   "source": [
    "Output type: KNN classifier is used for classification problems, where the output variable is categorical, whereas KNN regressor is used for regression problems, where the output variable is continuous.\n",
    "\n",
    "Performance metrics: The performance of KNN classifier is evaluated using classification metrics, such as accuracy, precision, recall, and F1-score, while the performance of KNN regressor is evaluated using regression metrics, such as mean squared error and R-squared.\n",
    "\n",
    "K value selection: The optimal value of k is selected differently for KNN classifier and regressor. For KNN classifier, a small k value is preferred to avoid overfitting, while for KNN regressor, a larger k value is preferred to reduce the effect of outliers.\n",
    "\n",
    "Data distribution: KNN classifier performs well when the data is well-separated, while KNN regressor performs well when the data is continuous and smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef2fa0-9617-440e-abc1-eac42d7614a2",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df5cfd-1e3e-43c6-bec0-d739a5446f90",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "1. Simple and intuitive: KNN is a simple and intuitive algorithm that is easy to understand and implement.\n",
    "\n",
    "2. Non-parametric: KNN is a non-parametric algorithm, which means it does not make any assumptions about the underlying data distribution.\n",
    "\n",
    "3. Can handle non-linear relationships: KNN can handle non-linear relationships between the input and output variables, making it suitable for a wide range of problems.\n",
    "\n",
    "4. No training time: KNN does not require any training time, which means it can be used for real-time predictions.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "1. Computationally expensive: KNN can be computationally expensive, especially when dealing with large datasets or high-dimensional data.\n",
    "\n",
    "2. Sensitivity to noise and outliers: KNN is sensitive to noise and outliers in the data, which can significantly affect its performance.\n",
    "\n",
    "3. Requires feature scaling: KNN requires feature scaling to ensure that all features have equal importance.\n",
    "\n",
    "4. Can be biased towards majority classes: KNN can be biased towards majority classes, especially when the classes are imbalanced.\n",
    "\n",
    "To address these weaknesses, several techniques can be used, such as:\n",
    "\n",
    "1. Dimensionality reduction: Dimensionality reduction techniques, such as principal component analysis (PCA), can be used to reduce the dimensionality of the data and improve the performance of KNN.\n",
    "\n",
    "2. Distance metric learning: Distance metric learning techniques can be used to learn a more appropriate distance metric for the data, which can reduce the effect of noise and outliers.\n",
    "\n",
    "3. Ensemble learning: Ensemble learning techniques, such as bagging and boosting, can be used to improve the robustness of KNN and reduce its sensitivity to noise and outliers.\n",
    "\n",
    "4. Cross-validation: Cross-validation techniques can be used to evaluate the performance of KNN and select the optimal value of k.\n",
    "\n",
    "5. Class weighting: Class weighting techniques can be used to address the bias towards majority classes and improve the performance of KNN on imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60934c05-e6a1-41c9-bdc7-d8e0a73128ba",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e685a7b-5105-47cb-8914-53f772d94aa9",
   "metadata": {},
   "source": [
    "Euclidean distance:\n",
    "\n",
    "1. Euclidean distance is the straight-line distance between two points in a Euclidean space.\n",
    "\n",
    "2. It measures the distance between two points using the square root of the sum of the squared differences between their coordinates.\n",
    "\n",
    "3. It is sensitive to the magnitude of the variables and can be affected by outliers.\n",
    "\n",
    "4. It is commonly used in KNN for continuous variables.\n",
    "\n",
    "Manhattan distance:\n",
    "\n",
    "1. Manhattan distance is the distance between two points measured along the axes at right angles.\n",
    "\n",
    "2. It measures the distance between two points using the sum of the absolute differences between their coordinates.\n",
    "\n",
    "3. It is less sensitive to the magnitude of the variables and can handle outliers better than Euclidean distance.\n",
    "\n",
    "4. It is commonly used in KNN for categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc72fd5-5c97-4a13-8193-8b7c8bf899b4",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e421e-8cd5-4fba-8763-c1a27c1ace5a",
   "metadata": {},
   "source": [
    "In K-Nearest Neighbors (KNN) algorithm, feature scaling is an important preprocessing step that is used to ensure that all features are equally important and have the same scale. The main role of feature scaling in KNN is to normalize the range of features, so that the distance between two points is not dominated by a single feature with a large range.\n",
    "\n",
    "Without feature scaling, the features with larger ranges can have a greater impact on the distance calculation and can dominate the KNN algorithm. This can lead to biased predictions and affect the performance of the model. Feature scaling helps to overcome this issue by bringing all features to the same scale, thus ensuring that all features contribute equally to the distance calculation.\n",
    "\n",
    "There are several methods for feature scaling, including:\n",
    "\n",
    "1. Min-Max scaling: This method scales the features to a range between 0 and 1. It is given by the formula:\n",
    "scaled_value = (value - min) / (max - min)\n",
    "\n",
    "2. Standardization: This method scales the features to have a mean of 0 and a standard deviation of 1. It is given by the formula:\n",
    "scaled_value = (value - mean) / standard_deviation\n",
    "\n",
    "Feature scaling should be applied before fitting the KNN model to the data, as it can significantly improve the performance of the algorithm. However, it is important to note that some distance metrics, such as Manhattan distance, are less sensitive to feature scaling and may not require it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccebcd-9063-4b81-8402-e91e7ad0ee0a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

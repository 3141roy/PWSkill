{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c05038-e9f5-48ea-87d2-55d807dd987b",
   "metadata": {},
   "source": [
    "#### Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d16d48-305a-4c22-897d-00f7c20efe52",
   "metadata": {},
   "source": [
    "Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from the expected behavior or normalcy within a dataset or system. It involves the detection of unusual or anomalous data points, events, or behaviors that do not conform to the expected patterns or trends.\n",
    "\n",
    "The purpose of anomaly detection is to identify and flag observations that are considered rare, suspicious, or potentially indicative of abnormal behavior or events.\n",
    "\n",
    "Unlike many ML domains sometimes Outliers possess important information and cannot be outrigh ignored/removed from the dataset like in Medical Test results or in detecting hacking via suspecious logins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf292131-1c10-4bcd-85a0-f133ce4ef03e",
   "metadata": {},
   "source": [
    "#### Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed40a4-7d7b-4b07-a070-61048c18f945",
   "metadata": {},
   "source": [
    "Anomaly detection presents several challenges that need to be addressed for effective implementation. Some key challenges in anomaly detection include:\n",
    "\n",
    "1. Lack of labeled data: Anomaly detection often requires labeled data for training supervised models. However, obtaining labeled data can be challenging, as anomalies are often rare and may not be adequately represented in the training dataset. Generating a sufficient amount of labeled anomaly data can be time-consuming, expensive, or impractical in certain domains.\n",
    "\n",
    "2. Imbalanced datasets: Anomalies are typically rare compared to normal instances, leading to imbalanced datasets where anomalies are underrepresented. This can pose challenges for anomaly detection algorithms that are optimized for balanced datasets. Imbalanced data can result in biased models, where the algorithm tends to prioritize normal instances and may have difficulty accurately detecting anomalies.\n",
    "\n",
    "3. Identifying novel anomalies: Anomaly detection algorithms are usually trained on known or previously observed anomalies. However, they may struggle to identify novel or previously unseen anomalies that differ significantly from the training data. The ability to generalize and detect unknown anomalies is a significant challenge.\n",
    "\n",
    "4. Feature engineering: Choosing appropriate features or representations that capture relevant information for anomaly detection is critical. In some cases, anomalies may be present in high-dimensional or complex data, making it challenging to identify meaningful features that effectively differentiate between normal and abnormal instances. Extracting informative features often requires domain expertise and extensive data exploration.\n",
    "\n",
    "5. Dynamic environments: Anomaly detection in dynamic environments where normal behavior or data distributions change over time presents challenges. Models trained on historical data may become outdated or fail to adapt to new patterns or anomalies that emerge. Continuous monitoring and updating of the anomaly detection system are necessary to handle such dynamic scenarios effectively.\n",
    "\n",
    "6. False positives and false negatives: Striking a balance between detecting anomalies accurately and minimizing false positives (normal instances misclassified as anomalies) and false negatives (anomalies not detected) can be challenging. The threshold for classifying instances as anomalies needs to be carefully tuned to achieve the desired balance, considering the cost of false alarms and missed detections in different applications.\n",
    "\n",
    "7. Scalability: Anomaly detection algorithms should be scalable to handle large and high-dimensional datasets in real-time or near-real-time scenarios. Efficient algorithms and techniques are required to process and analyze vast amounts of data quickly without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2e723-3747-4895-add8-932bc8c5f8c9",
   "metadata": {},
   "source": [
    "#### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b351df1-bcd9-4ac6-b0aa-cc6f9dcfdb60",
   "metadata": {},
   "source": [
    "There are two main categories of machine learning methods: supervised and unsupervised. In a nutshell, supervised machine learning algorithms are trained with examples. Humans feed them datasets containing examples which are already labeled or categorized, which enables the algorithm to build a general model of each category. The algorithm then processes the real (uncategorized) data and attempts to put each item into one of the pre-learned categories. Since a supervised algorithm only knows the categories on which it has been trained, and its training was conducted on pre-labeled examples, a supervised machine learning algorithm cannot place an item into a category it has not seen an example of. This means that an automated anomaly detection system built on such an algorithm would have to be given examples of every single possible type of anomaly on every possible data distribution, pattern and trend. As you can imagine, this is not always practical given the sheer quantity and distribution of data generated today.\n",
    "\n",
    "Unsupervised machine learning algorithms, on the other hand, learn what normal behavior is, and then apply a statistical technique to determine if a specific data point is an anomaly. A system based on this kind of anomaly detection technique is able to detect any type of anomaly, including ones which have never been seen before. The main challenge in using unsupervised machine learning methods for detecting anomalies is determining what is considered normal for a given time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff6acd-23b2-423d-a088-c03f539d5f25",
   "metadata": {},
   "source": [
    "#### Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a195097-be1a-4477-b9da-7b150d03428b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9819284f-799a-4d05-acaa-513433630250",
   "metadata": {},
   "source": [
    "#### Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c807e37-adc1-40de-bfe9-69389d3df793",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d48e94d5-dcc2-42d7-9635-444c2f573d0e",
   "metadata": {},
   "source": [
    "#### Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9cabb-35be-4f3e-928e-aa5a1f51775c",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores based on the concept of local density. It measures the deviation of a data point's density compared to its neighboring points. The algorithm follows the following steps to compute anomaly scores:\n",
    "\n",
    "1. `Define the Neighborhood`: For each data point, the LOF algorithm identifies its k nearest neighbors based on a chosen distance metric. The value of k is a user-defined parameter.\n",
    "\n",
    "2. `Calculate Local Reachability Density`: The local reachability density (LRD) of a data point is calculated by determining the inverse of the average reachability distance of its k nearest neighbors. The reachability distance between two points is the maximum of the distance between the points and the distance to the k-th nearest neighbor. The LRD measures the local density of a data point relative to its neighbors.\n",
    "\n",
    "3. `Calculate Local Outlier Factor`: The local outlier factor (LOF) of a data point is computed by comparing the LRD of the data point with the LRDs of its k nearest neighbors. The LOF quantifies the degree of abnormality of a data point compared to its neighbors. A high LOF value indicates that the data point is more isolated or less dense compared to its neighbors, suggesting it is an anomaly. The LOF is calculated as the average ratio of the LRD of a data point to the LRDs of its k nearest neighbors.\n",
    "\n",
    "4. `Normalize Anomaly Scores`: The LOF scores obtained for each data point are normalized to ensure they are on a consistent scale. This normalization step helps in interpreting and comparing the anomaly scores across different datasets.\n",
    "\n",
    "5. `Interpret Anomaly Scores`: The computed anomaly scores represent the degree of outlierness or abnormality of each data point. Higher anomaly scores indicate a higher likelihood of being an anomaly, while lower scores indicate a higher resemblance to the normal behavior of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b65159-d573-4459-a494-22499abed7c2",
   "metadata": {},
   "source": [
    "#### Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671ef2a-0a52-40ca-bca9-b617d0babd0d",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has several key parameters that can be adjusted to influence its behavior and performance. Here are the main parameters of the Isolation Forest algorithm:\n",
    "\n",
    "1. `Number of Trees (n_estimators)`: This parameter determines the number of isolation trees to be created in the forest. Increasing the number of trees can lead to a more accurate anomaly detection but also increases the computational cost.\n",
    "\n",
    "2. `Sample Size (max_samples)`: It determines the number of samples to be used when creating each isolation tree. A smaller sample size can speed up the algorithm but may affect the detection accuracy, while a larger sample size can improve accuracy but may increase computational overhead.\n",
    "\n",
    "3. `Maximum Tree Depth (max_depth)`: This parameter sets the maximum depth allowed for each isolation tree. Controlling the maximum tree depth can help prevent overfitting and limit the algorithm's sensitivity to noise. Setting a higher value can potentially capture more complex anomalies but also increases the risk of overfitting.\n",
    "\n",
    "4. `Contamination`: The contamination parameter represents the expected proportion of anomalies in the dataset. It provides a prior estimate of the anomaly rate, which can help in setting a suitable threshold for anomaly detection. By default, it is set to \"auto,\" allowing the algorithm to estimate the contamination automatically based on the dataset.\n",
    "\n",
    "5. `Random Seed (random_state)`: The random seed parameter allows for the reproducibility of results. By setting a fixed random seed, the algorithm's randomization process will generate the same set of trees each time, ensuring consistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e1ccb-4d8e-490e-bcd6-27df05b85c41",
   "metadata": {},
   "source": [
    "#### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0c25d-2cb6-47dd-bcb4-4018e7706b27",
   "metadata": {},
   "source": [
    "\n",
    "The anomaly score for a data point using KNN with K=10 is calculated as follows:\n",
    "\n",
    "anomaly_score = 1 - (number of neighbors of the same class / K)\n",
    "\n",
    "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5, and K=10. Therefore, the anomaly score is:\n",
    "\n",
    "anomaly_score = 1 - (2 / 10) = 0.8\n",
    "\n",
    "This means that the data point is 80% likely to be an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6efe5c-b2e8-46e3-b74f-875a0ab72a5b",
   "metadata": {},
   "source": [
    "#### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a506572-7f35-41d2-aa4e-8aef9b2d3b0a",
   "metadata": {},
   "source": [
    "The anomaly score in the Isolation Forest algorithm is calculated based on the average path length (APL) of a data point compared to the average path length of the trees in the forest. The APL represents the average number of edges traversed from the root to reach the data point in each tree. A lower APL indicates that the data point is isolated and likely to be an anomaly.\n",
    "\n",
    "To calculate the anomaly score for a data point with an average path length of 5.0 compared to the average path length of the trees in the Isolation Forest algorithm with 100 trees, we need to know the distribution of average path lengths in the forest.\n",
    "\n",
    "The anomaly score in the Isolation Forest algorithm is calculated as follows:\n",
    "\n",
    "anomaly score = 2^(-average path length / c)\n",
    "\n",
    "Where 'c' is a constant that depends on the number of data points in the dataset. In the original Isolation Forest paper, it is recommended to set 'c' to approximately (2 * H(n-1) / n), where 'H' is the harmonic number and 'n' is the number of data points.\n",
    "\n",
    "Since you mentioned a dataset of 3000 data points and an average path length of 5.0, we can estimate the anomaly score if we assume a specific value for 'c' based on the recommended formula mentioned above. However, it's important to note that without the actual distribution of average path lengths in the forest, this estimation may not be accurate.\n",
    "\n",
    "If we assume a value of 'c' based on the formula mentioned above, we can calculate the anomaly score as follows:\n",
    "\n",
    "anomaly score = 2^(-5.0 / c)\n",
    "\n",
    "Keep in mind that the accuracy and interpretation of the anomaly score may vary depending on the specific dataset, the distribution of average path lengths, and the chosen value of 'c'. It's recommended to validate and interpret the anomaly score in the context of your specific dataset and the characteristics of anomalies present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5f0fb-590a-4fd7-9e0a-b29983f11c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
